## References
[Generative Adversarial Text-to-Image Synthesis](https://arxiv.org/abs/1605.05396)

The code is adapted from [https://github.com/utsav-195/text-to-image-generator-gan](https://github.com/utsav-195/text-to-image-generator-gan)


The data of the image description was obtained from [here](https://drive.google.com/file/d/0B0ywwgffWnLLcms2WWJQRFNSWXM/view?resourcekey=0-Av8zFbeDDvNcF1sSjDR32w). The image caption data link was obtained from this [github](https://github.com/zsdonghao/text-to-image).

The text embedding is done using google [GoogleNews-vectors-negative300.bin](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g)

The text embedding and image embedding should be placed in 'embedding' folder.
The pre-trained model is in the folder models

Run the notebook DCGAN.ipynb for results. 





