{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91197f48",
   "metadata": {},
   "source": [
    "# Text to Image Generation using DCGAN Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "569177d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import imageio\n",
    "import os\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense, Concatenate \n",
    "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D, Conv2DTranspose\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import initializers\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486ae856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation resolution - Must be square \n",
    "# Training data is also scaled to this.\n",
    "GENERATE_RES = 2 # Generation resolution factor \n",
    "# (1=32, 2=64, 3=96, 4=128, etc.)\n",
    "GENERATE_SQUARE = 32 * GENERATE_RES # rows/cols (should be square)\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "# Preview image \n",
    "PREVIEW_ROWS = 4\n",
    "PREVIEW_COLS = 7\n",
    "PREVIEW_MARGIN = 16\n",
    "\n",
    "# Size vector to generate images from\n",
    "SEED_SIZE = 100\n",
    "EMBEDDING_SIZE = 300\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"./jpg\"\n",
    "MODEL_PATH = \" \"\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 4000\n",
    "\n",
    "print(f\"Will generate {GENERATE_SQUARE}px square images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9329a1",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "\n",
    "#### Image Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a44670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the model GoogleNews-vectors-negative300.bin using gensim for embedding\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69afb3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1c/d3n7ylmn71n9gw84l4xx1pp00000gn/T/ipykernel_56539/1009898140.py:19: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  image = Image.open(path).resize((64,64),Image.ANTIALIAS) # reducing the image size into 64px\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embedding finished and saving...\n",
      "Time taken to complete embedding: 41.230995178222656\n"
     ]
    }
   ],
   "source": [
    "# This section of code is for image embedding\n",
    "# Image set has 8,188 images, the image embeddings are saved as a numpy file.\n",
    "\n",
    "\n",
    "embedding_file = os.path.join('./training_data_/',\n",
    "        f'image_embedding') \n",
    "start = time.time()\n",
    "print(\"Loading training images...\")\n",
    "\n",
    "training_data = []\n",
    "# flowers_path = sorted(os.listdir(DATA_PATH))\n",
    "\n",
    "flowers_path = sorted(os.listdir('./flower_images/'))\n",
    "\n",
    "for filename in range(len(flowers_path)):\n",
    "    path = os.path.join('./flower_images/',flowers_path[filename])\n",
    "    # print(path)\n",
    "    try:\n",
    "      image = Image.open(path).resize((64,64),Image.ANTIALIAS) # reducing the image size into 64px\n",
    "      channel = np.asarray(image).shape[2]\n",
    "      if channel == 3:\n",
    "        training_data.append(np.asarray(image))\n",
    "    except:\n",
    "      pass\n",
    "training_data = np.reshape(training_data,(-1,64,64,3))     #reshaping numpy array into (64,64,3)\n",
    "training_data = training_data.astype(np.float32)\n",
    "     \n",
    "training_data = training_data / 127.5 - 1.            #Normalizing the input\n",
    "\n",
    "print(\"Image embedding finished and saving...\")\n",
    "np.save(embedding_file + \".npy\",training_data)\n",
    "\n",
    "print (f'Time taken to complete embedding: {time.time()-start}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b600e5c",
   "metadata": {},
   "source": [
    "### Text Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6f33728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text embedding completed and saved\n"
     ]
    }
   ],
   "source": [
    "# This section of code is for text embedding\n",
    "\n",
    "text_path = \"./text_c10/captions\"\n",
    "text_files = sorted(os.listdir(text_path))\n",
    "captions = []\n",
    "caption_embeddings = np.zeros((len(text_files),300),dtype=np.float32)\n",
    "for filename in range(len(text_files)):\n",
    "    path = os.path.join(text_path,text_files[filename])\n",
    "    # print(path)\n",
    "    f = open(path,'r')\n",
    "    data = f.read()\n",
    "    data = data.split(\"\\n\")\n",
    "    f.close()\n",
    "    for d in range(1):\n",
    "      x = data[d].lower()\n",
    "      x = x.replace(\"  \",\"\")\n",
    "      # x = x.replace(\"'\",\"\")\n",
    "      captions.append(x)\n",
    "      count = 0\n",
    "      for t in x:\n",
    "        try:\n",
    "          caption_embeddings[filename] += model[t]\n",
    "          count += 1\n",
    "        except:\n",
    "          pass\n",
    "      caption_embeddings[filename] /= count\n",
    "np.save('./training_data_/flowers_text_embedding.npy',caption_embeddings)\n",
    "print(\"text embedding completed and saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45d431c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>captions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>prominent purple stigma,petals are white inc olor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this flower is blue and green in color, with p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>outer petals are green in color and klarger,in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>there are several shapes, sizes, and colors of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the stamen are towering over the stigma which ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            captions\n",
       "0  prominent purple stigma,petals are white inc olor\n",
       "1  this flower is blue and green in color, with p...\n",
       "2  outer petals are green in color and klarger,in...\n",
       "3  there are several shapes, sizes, and colors of...\n",
       "4  the stamen are towering over the stigma which ..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The captions are stored as a csv file for later usage\n",
    "df_captions = pd.DataFrame([])\n",
    "df_captions['captions'] = captions\n",
    "df_captions.to_csv(\"./text_c10/flowercaptions.csv\",index=None)\n",
    "df_captions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bae6b1",
   "metadata": {},
   "source": [
    "### Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef60ae93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedded files are loaded\n",
    "\n",
    "caption_embeddings = np.load('./training_data_/flowers_text_embedding.npy')[:8128]\n",
    "image_embeddings = np.load('./training_data_/image_embedding.npy')[:8128]\n",
    "save_images_embeddings = np.copy(caption_embeddings[:28])\n",
    "save_images_npy = np.copy(image_embeddings[:28])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabbdcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the data as batches \n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices({'images': image_embeddings.astype('float32'),\n",
    "                                                    'embeddings': caption_embeddings.astype('float32')}).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c77c290",
   "metadata": {},
   "source": [
    "## Defining Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37db8e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_generator_func(seed_size,embedding_size, channels):\n",
    "  input_seed = Input(shape=seed_size)\n",
    "  input_embed = Input(shape = embedding_size)\n",
    "  d0 = Dense(128)(input_embed)\n",
    "  leaky0 = LeakyReLU(alpha=0.2)(d0)\n",
    "\n",
    "  merge = Concatenate()([input_seed, leaky0])\n",
    "\n",
    "  d1 = Dense(4*4*256,activation=\"relu\")(merge)\n",
    "  reshape = Reshape((4,4,256))(d1)\n",
    "\n",
    "  upSamp1 = UpSampling2D()(reshape)\n",
    "  conv2d1 = Conv2DTranspose(256,kernel_size=5,padding=\"same\",kernel_initializer=initializers.RandomNormal(stddev=0.02))(upSamp1)\n",
    "  batchNorm1 = BatchNormalization(momentum=0.8)(conv2d1)\n",
    "  leaky1 = LeakyReLU(alpha=0.2)(batchNorm1)\n",
    "\n",
    "  upSamp2 = UpSampling2D()(leaky1)\n",
    "  conv2d2 = Conv2DTranspose(256,kernel_size=5,padding=\"same\",kernel_initializer=initializers.RandomNormal(stddev=0.02))(upSamp2)\n",
    "  batchNorm2 = BatchNormalization(momentum=0.8)(conv2d2)\n",
    "  leaky2 = LeakyReLU(alpha=0.2)(batchNorm2)\n",
    "\n",
    "  upSamp3 = UpSampling2D()(leaky2)\n",
    "  conv2d3 = Conv2DTranspose(128,kernel_size=4,padding=\"same\",kernel_initializer=initializers.RandomNormal(stddev=0.02))(upSamp3)\n",
    "  batchNorm3 = BatchNormalization(momentum=0.8)(conv2d3)\n",
    "  leaky3 = LeakyReLU(alpha=0.2)(batchNorm3)\n",
    "\n",
    "  upSamp4 = UpSampling2D(size=(GENERATE_RES,GENERATE_RES))(leaky3)\n",
    "  conv2d4 = Conv2DTranspose(128,kernel_size=4,padding=\"same\",kernel_initializer=initializers.RandomNormal(stddev=0.02))(upSamp4)\n",
    "  batchNorm4 = BatchNormalization(momentum=0.8)(conv2d4)\n",
    "  leaky4 = LeakyReLU(alpha=0.2)(batchNorm4)\n",
    "\n",
    "  outputConv = Conv2DTranspose(channels,kernel_size=3,padding=\"same\",kernel_initializer=initializers.RandomNormal(stddev=0.02))(leaky4)\n",
    "  outputActi = Activation(\"tanh\")(outputConv)          # Activation function tanh() is used\n",
    "\n",
    "  model = Model(inputs=[input_seed,input_embed], outputs=outputActi)\n",
    "  return model\n",
    "\n",
    "def build_discriminator_func(image_shape, embedding_size):\n",
    "  input_shape = Input(shape=image_shape)\n",
    "  input_embed = Input(shape=embedding_size)\n",
    "\n",
    "  conv2d1 = Conv2D(32,kernel_size=4,strides=2,input_shape=image_shape,padding=\"same\",kernel_initializer=initializers.RandomNormal(stddev=0.02))(input_shape)\n",
    "  leaky1 = LeakyReLU(alpha=0.2)(conv2d1)\n",
    "\n",
    "  drop2 = Dropout(0.25)(leaky1)\n",
    "  conv2d2 = Conv2D(64, kernel_size=4, strides=2, padding=\"same\",kernel_initializer=initializers.RandomNormal(stddev=0.02))(drop2)\n",
    "  batchNorm2 = BatchNormalization(momentum=0.8)(conv2d2)\n",
    "  leaky2 = LeakyReLU(alpha=0.2)(batchNorm2)\n",
    "\n",
    "  drop3 = Dropout(0.25)(leaky2)\n",
    "  conv2d3 = Conv2D(128, kernel_size=4, strides=2, padding=\"same\",kernel_initializer=initializers.RandomNormal(stddev=0.02))(drop3)\n",
    "  batchNorm3 = BatchNormalization(momentum=0.8)(conv2d3)\n",
    "  leaky3 = LeakyReLU(alpha=0.2)(batchNorm3)\n",
    "\n",
    "  drop4 = Dropout(0.25)(leaky3)\n",
    "  conv2d4 = Conv2D(256, kernel_size=4, strides=2, padding=\"same\",kernel_initializer=initializers.RandomNormal(stddev=0.02))(drop4)\n",
    "  batchNorm4 = BatchNormalization(momentum=0.8)(conv2d4)\n",
    "  leaky4 = LeakyReLU(alpha=0.2)(batchNorm4)\n",
    "\n",
    "  dense_embed = Dense(128,kernel_initializer=initializers.RandomNormal(stddev=0.02))(input_embed)\n",
    "  leaky_embed = LeakyReLU(alpha=0.2)(dense_embed)\n",
    "  reshape_embed = Reshape((4,4,8))(leaky_embed)\n",
    "  merge_embed = Concatenate()([leaky4, reshape_embed])\n",
    "\n",
    "  drop5 = Dropout(0.25)(merge_embed)\n",
    "  conv2d5 = Conv2D(512, kernel_size=4,kernel_initializer=initializers.RandomNormal(stddev=0.02))(drop5)\n",
    "  batchNorm5 = BatchNormalization(momentum=0.8)(conv2d5)\n",
    "  leaky5 = LeakyReLU(alpha=0.2)(batchNorm5)\n",
    "\n",
    "  drop6 = Dropout(0.25)(leaky5)\n",
    "  flatten = Flatten()(drop6)\n",
    "  output = Dense(1,activation=\"sigmoid\")(flatten)              # Activation function sigmoid is used\n",
    "\n",
    "  model = Model(inputs=[input_shape,input_embed], outputs=output)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3de92557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initlializing a generator and discriminator\n",
    "\n",
    "image_shape = (GENERATE_SQUARE,GENERATE_SQUARE,IMAGE_CHANNELS)\n",
    "generator = build_generator_func(SEED_SIZE,EMBEDDING_SIZE, IMAGE_CHANNELS)\n",
    "discriminator = build_discriminator_func(image_shape,EMBEDDING_SIZE)\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=2.0e-4,beta_1 = 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=2.0e-4,beta_1 = 0.5)\n",
    "\n",
    "# generator.load_weights(\"/content/drive/Shareddrives/D4NLP Project/flowers data/flowers/model/text_to_image_generator_cub_character.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7dde52",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for finding the Inception Score\n",
    "# This section of code is adapted from https://machinelearningmastery.com/how-to-implement-the-inception-score-from-scratch-for-evaluating-generated-images/\n",
    "\n",
    "from math import floor\n",
    "from numpy import ones\n",
    "from numpy import expand_dims\n",
    "from numpy import log\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import exp\n",
    "from numpy.random import shuffle\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.datasets import cifar10\n",
    "from skimage.transform import resize\n",
    "from numpy import asarray\n",
    "\n",
    "# scale an array of images to a new size\n",
    "def scale_images(images, new_shape):\n",
    "\timages_list = list()\n",
    "\tfor image in images:\n",
    "\t\t# resize with nearest neighbor interpolation\n",
    "\t\tnew_image = resize(image, new_shape, 0)\n",
    "\t\t# store\n",
    "\t\timages_list.append(new_image)\n",
    "\treturn asarray(images_list)\n",
    "\n",
    "# assumes images have any shape and pixels in [0,255]\n",
    "def calculate_inception_score(images, n_split=10, eps=1E-16):\n",
    "\t# load inception v3 model\n",
    "\tmodel = InceptionV3()\n",
    "\t# enumerate splits of images/predictions\n",
    "\tscores = list()\n",
    "\tn_part = floor(images.shape[0] / n_split)\n",
    "\tfor i in range(n_split):\n",
    "\t\t# retrieve images\n",
    "\t\tix_start, ix_end = i * n_part, (i+1) * n_part\n",
    "\t\tsubset = images[ix_start:ix_end]\n",
    "\t\t# convert from uint8 to float32\n",
    "\t\tsubset = subset.astype('float32')\n",
    "\t\t# scale images to the required size\n",
    "\t\tsubset = scale_images(subset, (299,299,3))\n",
    "\t\t# pre-process images, scale to [-1,1]\n",
    "\t\tsubset = preprocess_input(subset)\n",
    "\t\t# predict p(y|x)\n",
    "\t\tp_yx = model.predict(subset)\n",
    "\t\t# calculate p(y)\n",
    "\t\tp_y = expand_dims(p_yx.mean(axis=0), 0)\n",
    "\t\t# calculate KL divergence using log probabilities\n",
    "\t\tkl_d = p_yx * (log(p_yx + eps) - log(p_y + eps))\n",
    "\t\t# sum over classes\n",
    "\t\tsum_kl_d = kl_d.sum(axis=1)\n",
    "\t\t# average over images\n",
    "\t\tavg_kl_d = mean(sum_kl_d)\n",
    "\t\t# undo the log\n",
    "\t\tis_score = exp(avg_kl_d)\n",
    "\t\t# store\n",
    "\t\tscores.append(is_score)\n",
    "\t# average across images\n",
    "\tis_avg, is_std = mean(scores), std(scores)\n",
    "\treturn is_avg, is_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea9f08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for saving the image at each epochs while training\n",
    "\n",
    "# inception_score = []\n",
    "# steps = []\n",
    "\n",
    "def save_images(cnt,noise,embeds,testing = False):\n",
    "  \n",
    "  image_array = np.full(( \n",
    "      \n",
    "       + (PREVIEW_ROWS * (GENERATE_SQUARE+PREVIEW_MARGIN)), \n",
    "      PREVIEW_MARGIN + (PREVIEW_COLS * (GENERATE_SQUARE+PREVIEW_MARGIN)), 3), \n",
    "      255, dtype=np.uint8)\n",
    "  if not testing:                                         # saving fake images while training\n",
    "    generated_images = generator.predict((noise,embeds))\n",
    "    generated_images = 0.5 * generated_images + 0.5       # De-normalising the images\n",
    "    \n",
    "    output_path = \"./fake\"\n",
    "    filename = os.path.join(output_path,f\"epoch-{cnt}.png\")\n",
    "    \n",
    "    \n",
    "    #################################################################################################\n",
    "    # This part of the cell is to plot the inception score : ( Contributed myself)\n",
    "\n",
    "    # is_avg, is_std = calculate_inception_score(generated_images)\n",
    "    # print('score', is_avg, is_std)\n",
    "    # inception_score.append(is_avg)\n",
    "    # steps.append(cnt+1)\n",
    "    # plt.plot(steps, inception_score)\n",
    "    # plt.xlabel('epochs')\n",
    "    # plt.ylabel('Inception Score')\n",
    "    # plt.title('Inception Score Vs Epochs')\n",
    "    # graphpath = os.path.join(\"./IS graphs\",f\"IS.png\")\n",
    "    # plt.savefig(graphpath)\n",
    "    # plt.clf()\n",
    "    #################################################################################################\n",
    "  \n",
    "  \n",
    "  else:                                                       # saving test images\n",
    "    output_path = \"predictions\"\n",
    "    filename = os.path.join(output_path,f\"fake-{cnt}.png\")\n",
    "  \n",
    "  image_count = 0\n",
    "  for row in range(PREVIEW_ROWS):\n",
    "      for col in range(PREVIEW_COLS):\n",
    "        r = row * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
    "        c = col * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
    "        image_array[r:r+GENERATE_SQUARE,c:c+GENERATE_SQUARE] = generated_images[image_count] * 255\n",
    "        image_count += 1\n",
    "\n",
    "  if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "  im = Image.fromarray(image_array)\n",
    "  im.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11b92efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating losses\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "def discriminator_loss(real_image_real_text, fake_image_real_text, real_image_fake_text):\n",
    "    real_loss = cross_entropy(tf.random.uniform(real_image_real_text.shape,0.8,1.0), real_image_real_text)\n",
    "    fake_loss = (cross_entropy(tf.random.uniform(fake_image_real_text.shape,0.0,0.2), fake_image_real_text) + \n",
    "                 cross_entropy(tf.random.uniform(real_image_fake_text.shape,0.0,0.2), real_image_fake_text))/2\n",
    "\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6b810ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function for training the models\n",
    "\n",
    "@tf.function\n",
    "def train_step(images,captions,fake_captions):\n",
    "  seed = tf.random.normal([BATCH_SIZE, SEED_SIZE],dtype=tf.float32)\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_images = generator((seed,captions), training=True)\n",
    "    real_image_real_text = discriminator((images,captions), training=True)\n",
    "    real_image_fake_text = discriminator((images,fake_captions), training=True)\n",
    "    fake_image_real_text = discriminator((generated_images,captions), training=True)\n",
    "\n",
    "    gen_loss = generator_loss(fake_image_real_text)\n",
    "    disc_loss = discriminator_loss(real_image_real_text, fake_image_real_text, real_image_fake_text)\n",
    "    # print(gen_loss)\n",
    "    # print(disc_loss)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(\\\n",
    "        gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(\\\n",
    "        disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(\n",
    "        gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(\n",
    "        gradients_of_discriminator, \n",
    "        discriminator.trainable_variables))\n",
    "  return gen_loss,disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe793687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training process is called by this function\n",
    "\n",
    "# g_loss_list,d_loss_list =[],[]\n",
    "def train(train_dataset, epochs):\n",
    "  fixed_seed = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, \n",
    "                                       SEED_SIZE))\n",
    "  fixed_embed = save_images_embeddings\n",
    "\n",
    "  start = time.time()\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    print(\"epoch start...\")\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    gen_loss_list = []\n",
    "    disc_loss_list = []\n",
    "\n",
    "    for batch in train_dataset[:-1]:\n",
    "      # train_batch = training_data[BATCH_SIZE*image_batch : BATCH_SIZE*image_batch + BATCH_SIZE]\n",
    "      # caption_batch = captions[BATCH_SIZE*image_batch : BATCH_SIZE*image_batch + BATCH_SIZE]\n",
    "      train_batch = batch['images']\n",
    "      caption_batch = batch['embeddings']\n",
    "      \n",
    "      fake_caption_batch = np.copy(caption_batch)\n",
    "      np.random.shuffle(fake_caption_batch)\n",
    "      \n",
    "      t = train_step(train_batch,caption_batch,fake_caption_batch)\n",
    "      # print(t)\n",
    "      gen_loss_list.append(t[0])\n",
    "      disc_loss_list.append(t[1])\n",
    "      # if image_batch%50 == 0:\n",
    "      #   print(image_batch)\n",
    "      # print(\"here\")\n",
    "    print(\"now\")\n",
    "    g_loss = sum(gen_loss_list) / len(gen_loss_list)\n",
    "    d_loss = sum(disc_loss_list) / len(disc_loss_list)\n",
    "\n",
    "    # g_loss_list.append(g_loss)\n",
    "    # d_loss_list.append(d_loss)\n",
    "\n",
    "    save_images(epoch,fixed_seed,fixed_embed)\n",
    "\n",
    "    #################################################################################################\n",
    "    # This part of the code is to plot the loss graph (my contribution)\n",
    "    \n",
    "    # plt.plot(steps, g_loss_list,label = 'Generator loss')\n",
    "    # plt.plot(steps,d_loss_list, label = 'Discriminator loss')\n",
    "    # plt.xlabel('Epochs')\n",
    "    # plt.ylabel('Avg Loss')\n",
    "    # plt.title('Loss Vs Epochs')\n",
    "    # plt.legend()\n",
    "    # losspath = os.path.join(\"./loss graphs\",f\"loss.png\")\n",
    "    # plt.savefig(losspath)\n",
    "    # plt.clf()\n",
    "    #################################################################################################\n",
    "    \n",
    "    generator.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    discriminator.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    generator.save(os.path.join('./models',\"flower_gen.h5\"))\n",
    "    discriminator.save(os.path.join('./models',\"flower_disc.h5\"))\n",
    "    print(\"model saved\")\n",
    "    print(f'Epoch {epoch+1}, gen loss={g_loss},disc loss={d_loss}, {time.time()-epoch_start}')\n",
    "\n",
    "  \n",
    "  print ('Total Training time:', time.time()-start)\n",
    "\n",
    "\n",
    "train(list(train_dataset.as_numpy_iterator()), 500)      # loading the dataset to train function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e305887",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab6022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function for calculating the inception score\n",
    "# This section of code is adapted from https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/\n",
    "\n",
    "from numpy import iscomplexobj\n",
    "from scipy.linalg import sqrtm\n",
    "from numpy import trace\n",
    "from numpy import cov\n",
    "\n",
    "\n",
    "\n",
    "fid_model = InceptionV3(include_top=False, pooling='avg', input_shape=(299,299,3))\n",
    "# scale an array of images to a new size\n",
    "def scale_images(images, new_shape):\n",
    "\timages_list = list()\n",
    "\tfor image in images:\n",
    "\t\t# resize with nearest neighbor interpolation\n",
    "\t\tnew_image = resize(image, new_shape, 0)\n",
    "\t\t# store\n",
    "\t\timages_list.append(new_image)\n",
    "\treturn asarray(images_list)\n",
    " \n",
    "# calculate frechet inception distance\n",
    "def calculate_fid(model, images1, images2):\n",
    "  \n",
    "    images1 = images1.astype('float32')\n",
    "    images2 = images2.astype('float32')\n",
    "    # resize images\n",
    "    images1 = scale_images(images1, (299,299,3))\n",
    "    images2 = scale_images(images2, (299,299,3))\n",
    "    print('Scaled', images1.shape, images2.shape)\n",
    "    # pre-process images\n",
    "    images1 = preprocess_input(images1)\n",
    "    images2 = preprocess_input(images2)\n",
    "\n",
    "    # calculate activations\n",
    "    act1 = model.predict(images1)\n",
    "    act2 = model.predict(images2)\n",
    "    # calculate mean and covariance statistics\n",
    "    mu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
    "    mu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
    "    # calculate sum squared difference between means\n",
    "    ssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "    # calculate sqrt of product between cov\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    # check and correct imaginary numbers from sqrt\n",
    "    if iscomplexobj(covmean):\n",
    "      covmean = covmean.real\n",
    "    # calculate score\n",
    "    fid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a09f62e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pretrained generator model\n",
    "gen_model = tf.keras.models.load_model('./models/flower_gen.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e352240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function to test the model with new inputs\n",
    "\n",
    "def test_image(text,num):\n",
    "  test_embeddings = np.zeros((1,300),dtype=np.float32)\n",
    "\n",
    "  x = text.lower()\n",
    "  x = x.replace(\"  \",\"\")\n",
    "  count = 0\n",
    "  for t in x:\n",
    "    try:\n",
    "      test_embeddings[0] += model[t]\n",
    "      count += 1\n",
    "    except:\n",
    "      print(t)\n",
    "      pass\n",
    "  test_embeddings[0] /= count\n",
    "  test_embeddings =  np.repeat(test_embeddings,28,axis=0)\n",
    "  noise = tf.random.normal([28, 100])\n",
    "  save_images(num,noise,test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f33a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image(\"this flower is yellow in color\",600)\n",
    "# test_image(\"this flower is very much yellow in color\",305)\n",
    "# test_image(\"this flower is purple in color\",306)\n",
    "# test_image(\"this flower has clusters of orange and red petals surrounding brown-tinted stamen\",309)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b704162a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the predicted image\n",
    "import IPython\n",
    "IPython.display.Image('./predictions/fake-600.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mytension')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e6c34e26e911754d9e5d8ad53c6c4b28f3222f166c3c6db35ab8b1fb05f9dc4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
